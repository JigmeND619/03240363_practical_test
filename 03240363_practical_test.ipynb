{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem 1: Q-Learning on GridWorld - COMPLETE SOLUTION\n",
    "\n",
    "This solution implements Q-learning to solve a 5x5 GridWorld environment.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    5x5 GridWorld Environment\n",
    "\n",
    "    S . . . .\n",
    "    . # . # .\n",
    "    . . . # .\n",
    "    . # . . .\n",
    "    . . . . G\n",
    "\n",
    "    S = Start (0,0)\n",
    "    G = Goal (4,4)\n",
    "    # = Wall (cannot pass)\n",
    "    . = Empty cell\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grid_size = 5\n",
    "        self.walls = [(1,1), (1,3), (2,3), (3,1)]\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (4, 4)\n",
    "        self.current_pos = self.start\n",
    "\n",
    "        # Create mapping of valid positions to state indices\n",
    "        self.valid_positions = []\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if (i, j) not in self.walls:\n",
    "                    self.valid_positions.append((i, j))\n",
    "\n",
    "        self.n_states = len(self.valid_positions)\n",
    "        print(f\"GridWorld initialized with {self.n_states} valid states\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset to start position. Return initial state.\"\"\"\n",
    "        self.current_pos = self.start\n",
    "        return self.state_to_index(self.start)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action and return (next_state, reward, done)\n",
    "        Actions: 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
    "        \"\"\"\n",
    "        row, col = self.current_pos\n",
    "\n",
    "        # Calculate new position based on action\n",
    "        if action == 0:  # UP\n",
    "            new_pos = (row - 1, col)\n",
    "        elif action == 1:  # DOWN\n",
    "            new_pos = (row + 1, col)\n",
    "        elif action == 2:  # LEFT\n",
    "            new_pos = (row, col - 1)\n",
    "        else:  # RIGHT (action == 3)\n",
    "            new_pos = (row, col + 1)\n",
    "\n",
    "        # Check if new position is valid\n",
    "        new_row, new_col = new_pos\n",
    "\n",
    "        # Hit wall or boundary - stay in place, negative reward\n",
    "        if (new_pos in self.walls or\n",
    "            new_row < 0 or new_row >= self.grid_size or\n",
    "            new_col < 0 or new_col >= self.grid_size):\n",
    "            reward = -1\n",
    "            new_pos = self.current_pos\n",
    "        # Move to goal - positive reward\n",
    "        elif new_pos == self.goal:\n",
    "            reward = 10\n",
    "            self.current_pos = new_pos\n",
    "            return self.state_to_index(new_pos), reward, True\n",
    "        # Normal move - small negative reward (encourages efficiency)\n",
    "        else:\n",
    "            reward = -0.1\n",
    "            self.current_pos = new_pos\n",
    "\n",
    "        next_state = self.state_to_index(self.current_pos)\n",
    "        done = (self.current_pos == self.goal)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Convert (row, col) to unique state index\"\"\"\n",
    "        try:\n",
    "            return self.valid_positions.index(state)\n",
    "        except ValueError:\n",
    "            # Invalid state (wall)\n",
    "            return -1\n",
    "\n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"Convert state index back to (row, col)\"\"\"\n",
    "        if 0 <= index < len(self.valid_positions):\n",
    "            return self.valid_positions[index]\n",
    "        return None\n",
    "\n",
    "\n",
    "def train_qlearning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Train Q-learning agent\n",
    "\n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        episodes: Number of training episodes\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        epsilon: Exploration rate\n",
    "\n",
    "    Returns:\n",
    "        Q: Q-table (numpy array)\n",
    "        rewards: List of total rewards per episode\n",
    "    \"\"\"\n",
    "    n_states = env.n_states\n",
    "    n_actions = 4\n",
    "\n",
    "    # Initialize Q-table with zeros\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    # Track rewards\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    print(f\"Training Q-learning for {episodes} episodes...\")\n",
    "    print(f\"Parameters: alpha={alpha}, gamma={gamma}, epsilon={epsilon}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        max_steps = 100  # Prevent infinite loops\n",
    "\n",
    "        while steps < max_steps:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                # Explore: random action\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                # Exploit: best action from Q-table\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Q-learning update rule:\n",
    "            # Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] = Q[state, action] + alpha * td_error\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_per_episode[-100:])\n",
    "            print(f\"Episode {episode + 1}: Avg Reward (100) = {avg_reward:.2f}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Final average reward (last 100): {np.mean(rewards_per_episode[-100:]):.2f}\")\n",
    "\n",
    "    return Q, rewards_per_episode\n",
    "\n",
    "\n",
    "def visualize_policy(env, Q):\n",
    "    \"\"\"Visualize learned policy as arrows\"\"\"\n",
    "    action_symbols = ['â†‘', 'â†“', 'â†', 'â†’']\n",
    "\n",
    "    print(\"\\nLearned Policy:\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    for i in range(env.grid_size):\n",
    "        row_display = []\n",
    "        for j in range(env.grid_size):\n",
    "            if (i, j) == env.start:\n",
    "                row_display.append('S')\n",
    "            elif (i, j) == env.goal:\n",
    "                row_display.append('G')\n",
    "            elif (i, j) in env.walls:\n",
    "                row_display.append('#')\n",
    "            else:\n",
    "                state_idx = env.state_to_index((i, j))\n",
    "                if state_idx >= 0:\n",
    "                    best_action = np.argmax(Q[state_idx])\n",
    "                    row_display.append(action_symbols[best_action])\n",
    "                else:\n",
    "                    row_display.append('?')\n",
    "\n",
    "        print('  '.join(row_display))\n",
    "\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "\n",
    "def plot_results(rewards):\n",
    "    \"\"\"Plot training rewards\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot 1: Raw rewards\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards, alpha=0.6)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Rewards per Episode')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Moving average\n",
    "    plt.subplot(1, 2, 2)\n",
    "    window = 50\n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = [np.mean(rewards[max(0, i-window+1):i+1])\n",
    "                     for i in range(len(rewards))]\n",
    "        plt.plot(moving_avg, linewidth=2)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title(f'Moving Average ({window} episodes)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('problem1_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nPlot saved as 'problem1_results.png'\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def test_agent(env, Q, num_tests=10):\n",
    "    \"\"\"Test the trained agent\"\"\"\n",
    "    print(f\"\\nTesting agent for {num_tests} episodes...\")\n",
    "\n",
    "    successes = 0\n",
    "    total_steps = []\n",
    "\n",
    "    for test in range(num_tests):\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        max_steps = 100\n",
    "\n",
    "        while steps < max_steps:\n",
    "            # Use greedy policy (no exploration)\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, done = env.step(action)\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                successes += 1\n",
    "                total_steps.append(steps)\n",
    "                break\n",
    "\n",
    "    print(f\"Success rate: {successes}/{num_tests} ({100*successes/num_tests:.1f}%)\")\n",
    "    if total_steps:\n",
    "        print(f\"Average steps to goal: {np.mean(total_steps):.1f}\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"PROBLEM 1: Q-LEARNING ON GRIDWORLD\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Create environment\n",
    "    env = GridWorld()\n",
    "\n",
    "    # Train agent\n",
    "    Q, rewards = train_qlearning(\n",
    "        env,\n",
    "        episodes=1000,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.1\n",
    "    )\n",
    "\n",
    "    # Visualize policy\n",
    "    visualize_policy(env, Q)\n",
    "\n",
    "    # Plot results\n",
    "    plot_results(rewards)\n",
    "\n",
    "    # Test agent\n",
    "    test_agent(env, Q, num_tests=20)\n",
    "\n",
    "    # Print Q-table statistics\n",
    "    print(f\"\\nQ-table Statistics:\")\n",
    "    print(f\"  Shape: {Q.shape}\")\n",
    "    print(f\"  Non-zero values: {np.count_nonzero(Q)}/{Q.size}\")\n",
    "    print(f\"  Max Q-value: {np.max(Q):.2f}\")\n",
    "    print(f\"  Min Q-value: {np.min(Q):.2f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROBLEM 1 COMPLETE!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4479e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem 2: Deep Q-Network Completion - COMPLETE SOLUTION\n",
    "\n",
    "This solution completes the DQN implementation for CartPole-v1\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network architecture\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Neural network architecture\n",
    "        # Input: state_dim -> Hidden: 128 -> Hidden: 128 -> Output: action_dim\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer\"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample random mini-batch\"\"\"\n",
    "        # Randomly sample batch_size transitions\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Unpack and convert to numpy arrays\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # Initialize Q-network and target network\n",
    "        self.q_network = DQN(state_dim, action_dim)\n",
    "        self.target_network = DQN(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            # Exploit: best action from Q-network\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def train_step(self, batch_size=64):\n",
    "        \"\"\"Training step using experience replay\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return 0\n",
    "\n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Compute current Q-values\n",
    "        # Q(s,a) for the actions that were taken\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q-values using target network\n",
    "        # Target: r + Î³ * max_a' QÌ‚(s', a')\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss (MSE between current and target Q-values)\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "        # Backpropagate and update network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from Q-network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "def train_cartpole(episodes=500):\n",
    "    \"\"\"Train DQN on CartPole-v1\"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"PROBLEM 2: DQN ON CARTPOLE-V1\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    print(f\"Environment: CartPole-v1\")\n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Create agent\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "    # Training metrics\n",
    "    rewards = []\n",
    "    losses = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment\n",
    "        result = env.reset()\n",
    "        if isinstance(result, tuple):\n",
    "            state = result[0]\n",
    "        else:\n",
    "            state = result\n",
    "\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state, training=True)\n",
    "\n",
    "            # Take step in environment\n",
    "            result = env.step(action)\n",
    "            if len(result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            # Train agent\n",
    "            loss = agent.train_step()\n",
    "            if loss > 0:\n",
    "                episode_losses.append(loss)\n",
    "\n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        # Update target network every 10 episodes\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        # Track rewards\n",
    "        rewards.append(total_reward)\n",
    "        if episode_losses:\n",
    "            losses.append(np.mean(episode_losses))\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards)\n",
    "            print(f\"Episode {episode + 1}/{episodes}\")\n",
    "            print(f\"  Reward: {total_reward:.1f} | Avg (100): {avg_reward:.1f}\")\n",
    "            print(f\"  Steps: {steps} | Epsilon: {agent.epsilon:.3f}\")\n",
    "            if losses:\n",
    "                print(f\"  Loss: {losses[-1]:.4f}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # Check if solved\n",
    "        if len(rewards) >= 100 and np.mean(rewards[-100:]) >= 475:\n",
    "            print(f\"\\nðŸŽ‰ CartPole solved in {episode + 1} episodes!\")\n",
    "            print(f\"Average reward (100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Plot results\n",
    "    plot_results(rewards, losses)\n",
    "\n",
    "    return agent, rewards\n",
    "\n",
    "\n",
    "def plot_results(rewards, losses):\n",
    "    \"\"\"Plot training results\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot rewards\n",
    "    axes[0].plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    if len(rewards) >= 100:\n",
    "        moving_avg = [np.mean(rewards[max(0, i-99):i+1]) for i in range(len(rewards))]\n",
    "        axes[0].plot(moving_avg, linewidth=2, label='Moving Avg (100)')\n",
    "    axes[0].axhline(y=475, color='r', linestyle='--', label='Solved Threshold')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Reward')\n",
    "    axes[0].set_title('Training Rewards')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot losses\n",
    "    if losses:\n",
    "        axes[1].plot(losses, alpha=0.6, label='Loss')\n",
    "        if len(losses) >= 50:\n",
    "            moving_avg = [np.mean(losses[max(0, i-49):i+1]) for i in range(len(losses))]\n",
    "            axes[1].plot(moving_avg, linewidth=2, label='Moving Avg (50)')\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].set_title('Training Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('problem2_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nPlot saved as 'problem2_results.png'\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def test_agent(agent, episodes=10):\n",
    "    \"\"\"Test trained agent\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "    print(f\"\\nTesting agent for {episodes} episodes...\")\n",
    "    test_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        result = env.reset()\n",
    "        if isinstance(result, tuple):\n",
    "            state = result[0]\n",
    "        else:\n",
    "            state = result\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state, training=False)\n",
    "            result = env.step(action)\n",
    "\n",
    "            if len(result) == 5:\n",
    "                state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                state, reward, done, _ = result\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        test_rewards.append(total_reward)\n",
    "        print(f\"Test Episode {episode + 1}: Reward = {total_reward:.1f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Average: {np.mean(test_rewards):.2f}\")\n",
    "    print(f\"  Std Dev: {np.std(test_rewards):.2f}\")\n",
    "    print(f\"  Min/Max: {np.min(test_rewards):.1f} / {np.max(test_rewards):.1f}\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Train agent\n",
    "    agent, rewards = train_cartpole(episodes=500)\n",
    "\n",
    "    # Test agent\n",
    "    test_agent(agent, episodes=10)\n",
    "\n",
    "    # Final statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total episodes: {len(rewards)}\")\n",
    "    print(f\"Final avg reward (100): {np.mean(rewards[-100:]):.2f}\")\n",
    "    print(f\"Best episode reward: {np.max(rewards):.1f}\")\n",
    "    print(f\"Solved: {'Yes' if np.mean(rewards[-100:]) >= 475 else 'No'}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"PROBLEM 2 COMPLETE!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99728d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem 3: Analysis and Debugging - COMPLETE SOLUTION\n",
    "\n",
    "This solution identifies and fixes bugs in the FrozenLake Q-learning implementation\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# BUG ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "BUG 1: Q-table initialization\n",
    "Location: Line 15\n",
    "Description: Q-table is initialized but might have dimension issues\n",
    "Impact: Incorrect Q-table shape could cause indexing errors\n",
    "Fix: Ensure proper dimensions match state and action spaces\n",
    "\n",
    "BUG 2: Action selection (Line 33)\n",
    "Location: Line 33 (np.argmin instead of np.argmax)\n",
    "Description: Using argmin (minimum Q-value) instead of argmax (maximum Q-value)\n",
    "Impact: Agent chooses WORST actions instead of BEST actions - completely wrong!\n",
    "Fix: Change np.argmin(Q[state]) to np.argmax(Q[state])\n",
    "\n",
    "BUG 3: Q-learning update rule (Line 40)\n",
    "Location: Line 40\n",
    "Description: Missing learning rate (alpha) in the update rule\n",
    "             Current: Q[state, action] = reward + gamma * np.max(Q[next_state])\n",
    "             Correct: Q[state, action] += alpha * (target - current_q)\n",
    "Impact: Overwrites Q-values instead of gradually updating them\n",
    "Fix: Use proper temporal difference update with learning rate\n",
    "\n",
    "BUG 4: Epsilon decay (Line 51)\n",
    "Location: No epsilon decay implemented\n",
    "Description: Epsilon stays constant at 0.3, never decreases\n",
    "Impact: Agent never shifts from exploration to exploitation\n",
    "Fix: Add epsilon decay: epsilon = max(epsilon_min, epsilon * decay_rate)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# BUGGY CODE (For Reference)\n",
    "# ============================================================================\n",
    "\n",
    "def buggy_train_frozenlake(episodes=10000):\n",
    "    \"\"\"\n",
    "    BUGGY VERSION - For demonstration purposes\n",
    "    \"\"\"\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Bug 1: Q-table initialization (this is actually OK, but could note dimensions)\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    alpha = 0.8  # Learning rate\n",
    "    gamma = 0.95  # Discount factor\n",
    "    epsilon = 0.3  # Exploration rate\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Bug 2: Action selection issue - using argmin instead of argmax!\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmin(Q[state])  # BUG: Should be argmax!\n",
    "\n",
    "            result = env.step(action)\n",
    "            if len(result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "\n",
    "            # Bug 3: Update rule issue - missing learning rate and current Q-value\n",
    "            Q[state, action] = reward + gamma * np.max(Q[next_state])  # BUG: Wrong update!\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # Bug 4: Epsilon handling - no epsilon decay!\n",
    "\n",
    "    success_rate = np.mean(rewards[-100:])\n",
    "    print(f\"Buggy version success rate: {success_rate}\")\n",
    "\n",
    "    return Q, rewards\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED CODE\n",
    "# ============================================================================\n",
    "\n",
    "def fixed_train_frozenlake(episodes=10000):\n",
    "    \"\"\"\n",
    "    FIXED VERSION - All bugs corrected\n",
    "    \"\"\"\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    print(f\"FrozenLake Environment:\")\n",
    "    print(f\"  States: {n_states}\")\n",
    "    print(f\"  Actions: {n_actions}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # FIX 1: Q-table initialization (this was OK, but adding comment)\n",
    "    Q = np.zeros((n_states, n_actions))  # Properly shaped Q-table\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.1  # Learning rate (lowered from 0.8 for more stable learning)\n",
    "    gamma = 0.99  # Discount factor (increased for better long-term planning)\n",
    "    epsilon = 1.0  # Initial exploration rate (start high)\n",
    "    epsilon_min = 0.01  # Minimum exploration rate\n",
    "    epsilon_decay = 0.9995  # Epsilon decay rate\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    print(f\"Training with:\")\n",
    "    print(f\"  Learning rate (alpha): {alpha}\")\n",
    "    print(f\"  Discount factor (gamma): {gamma}\")\n",
    "    print(f\"  Initial epsilon: {epsilon}\")\n",
    "    print(f\"  Epsilon decay: {epsilon_decay}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        max_steps = 100  # Prevent infinite loops\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # FIX 2: Action selection - use argmax for exploitation!\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)  # Explore\n",
    "            else:\n",
    "                action = np.argmax(Q[state])  # Exploit (FIXED: was argmin)\n",
    "\n",
    "            result = env.step(action)\n",
    "            if len(result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "\n",
    "            # FIX 3: Proper Q-learning update rule with learning rate\n",
    "            # Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]\n",
    "            current_q = Q[state, action]\n",
    "            max_next_q = np.max(Q[next_state])\n",
    "            target_q = reward + gamma * max_next_q\n",
    "            Q[state, action] = current_q + alpha * (target_q - current_q)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # FIX 4: Epsilon decay - gradually reduce exploration\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            success_rate = np.mean(rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}: Success rate (100) = {success_rate:.3f}, Epsilon = {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    success_rate = np.mean(rewards[-100:])\n",
    "    print(f\"\\nFinal success rate: {success_rate:.3f}\")\n",
    "\n",
    "    return Q, rewards\n",
    "\n",
    "\n",
    "def compare_buggy_vs_fixed():\n",
    "    \"\"\"\n",
    "    Compare buggy and fixed implementations\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"PROBLEM 3: DEBUGGING Q-LEARNING\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nBUG ANALYSIS:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Bug 1: Q-table initialization\")\n",
    "    print(\"  Issue: Actually OK, but should verify dimensions\")\n",
    "    print(\"  Impact: Low\")\n",
    "    print(\"  Fix: Ensure Q = np.zeros((n_states, n_actions))\")\n",
    "    print()\n",
    "    print(\"Bug 2: Action selection (CRITICAL)\")\n",
    "    print(\"  Issue: Using np.argmin instead of np.argmax\")\n",
    "    print(\"  Impact: Agent chooses WORST actions, not BEST\")\n",
    "    print(\"  Fix: Change to np.argmax(Q[state])\")\n",
    "    print()\n",
    "    print(\"Bug 3: Q-learning update rule (CRITICAL)\")\n",
    "    print(\"  Issue: Missing learning rate, overwrites Q-values\")\n",
    "    print(\"  Impact: No gradual learning, unstable updates\")\n",
    "    print(\"  Fix: Q[s,a] += alpha * (target - Q[s,a])\")\n",
    "    print()\n",
    "    print(\"Bug 4: Epsilon decay (IMPORTANT)\")\n",
    "    print(\"  Issue: Epsilon never decreases\")\n",
    "    print(\"  Impact: Agent never exploits learned knowledge\")\n",
    "    print(\"  Fix: epsilon = max(epsilon_min, epsilon * decay)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    episodes = 5000\n",
    "\n",
    "    # Train buggy version\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training BUGGY version...\")\n",
    "    print(\"=\"*60)\n",
    "    Q_buggy, rewards_buggy = buggy_train_frozenlake(episodes=episodes)\n",
    "\n",
    "    # Train fixed version\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training FIXED version...\")\n",
    "    print(\"=\"*60)\n",
    "    Q_fixed, rewards_fixed = fixed_train_frozenlake(episodes=episodes)\n",
    "\n",
    "    # Compare results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBuggy Implementation:\")\n",
    "    print(f\"  Success rate: {np.mean(rewards_buggy[-100:]):.3f}\")\n",
    "    print(f\"  Total successes: {sum(rewards_buggy)}\")\n",
    "    print(f\"\\nFixed Implementation:\")\n",
    "    print(f\"  Success rate: {np.mean(rewards_fixed[-100:]):.3f}\")\n",
    "    print(f\"  Total successes: {sum(rewards_fixed)}\")\n",
    "    print(f\"\\nImprovement: {(np.mean(rewards_fixed[-100:]) - np.mean(rewards_buggy[-100:])):.3f}\")\n",
    "\n",
    "    # Plot comparison\n",
    "    plot_comparison(rewards_buggy, rewards_fixed)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROBLEM 3 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "def plot_comparison(rewards_buggy, rewards_fixed):\n",
    "    \"\"\"Plot comparison of buggy vs fixed implementation\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    window = 100\n",
    "\n",
    "    # Moving averages\n",
    "    ma_buggy = [np.mean(rewards_buggy[max(0, i-window+1):i+1])\n",
    "                for i in range(len(rewards_buggy))]\n",
    "    ma_fixed = [np.mean(rewards_fixed[max(0, i-window+1):i+1])\n",
    "                for i in range(len(rewards_fixed))]\n",
    "\n",
    "    # Plot 1: Success rates over time\n",
    "    axes[0].plot(ma_buggy, label='Buggy Version', alpha=0.7, linewidth=2)\n",
    "    axes[0].plot(ma_fixed, label='Fixed Version', alpha=0.7, linewidth=2)\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Success Rate (100-episode window)')\n",
    "    axes[0].set_title('Learning Progress Comparison')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Final performance comparison\n",
    "    final_buggy = np.mean(rewards_buggy[-100:])\n",
    "    final_fixed = np.mean(rewards_fixed[-100:])\n",
    "\n",
    "    axes[1].bar(['Buggy', 'Fixed'], [final_buggy, final_fixed],\n",
    "                color=['red', 'green'], alpha=0.7)\n",
    "    axes[1].set_ylabel('Success Rate')\n",
    "    axes[1].set_title('Final Performance (last 100 episodes)')\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    axes[1].text(0, final_buggy + 0.02, f'{final_buggy:.3f}',\n",
    "                ha='center', fontweight='bold')\n",
    "    axes[1].text(1, final_fixed + 0.02, f'{final_fixed:.3f}',\n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('problem3_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nComparison plot saved as 'problem3_comparison.png'\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    compare_buggy_vs_fixed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
